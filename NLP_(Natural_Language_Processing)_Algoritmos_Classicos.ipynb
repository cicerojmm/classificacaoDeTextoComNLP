{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP (Natural Language Processing) - Algoritmos Classicos.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ko6X5Nyt7dLp",
        "colab_type": "text"
      },
      "source": [
        "### Problema\n",
        "\n",
        "\n",
        "Uma revista precisa catalogar todas as suas notícias em diferentes categorias. O objetivo desta competição é desenvolver o melhor modelo de aprendizagem profunda para prever a categoria de novas notícias.\n",
        "\n",
        "\n",
        "<img src=\"https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/04/Untitled-Diagram.png\n",
        "\" style=\"width: 400px;\"/>\n",
        "\n",
        "\n",
        "As categorias possíveis são:\n",
        "\n",
        "* ambiente\n",
        "* equilibrioesaude\n",
        "* sobretudo\n",
        "* educacao\n",
        "* ciencia\n",
        "* tec\n",
        "* turismo\n",
        "* empreendedorsocial\n",
        "* comida"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5X7rqqeD4Uz",
        "colab_type": "code",
        "outputId": "8c8f5292-6fc7-4451-fc71-fef9a72f2150",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "!pip install unidecode"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting unidecode\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/42/d9edfed04228bacea2d824904cae367ee9efd05e6cce7ceaaedd0b0ad964/Unidecode-1.1.1-py2.py3-none-any.whl (238kB)\n",
            "\r\u001b[K     |█▍                              | 10kB 36.0MB/s eta 0:00:01\r\u001b[K     |██▊                             | 20kB 6.0MB/s eta 0:00:01\r\u001b[K     |████▏                           | 30kB 8.6MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 40kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 51kB 6.9MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 61kB 8.2MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 71kB 9.4MB/s eta 0:00:01\r\u001b[K     |███████████                     | 81kB 10.5MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 92kB 11.7MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 102kB 9.2MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 112kB 9.2MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 122kB 9.2MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 133kB 9.2MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 143kB 9.2MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 153kB 9.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 163kB 9.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 174kB 9.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 184kB 9.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 194kB 9.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 204kB 9.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 215kB 9.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 225kB 9.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 235kB 9.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 245kB 9.2MB/s \n",
            "\u001b[?25hInstalling collected packages: unidecode\n",
            "Successfully installed unidecode-1.1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8xZfZTqu3g6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn import decomposition, ensemble\n",
        "\n",
        "import pandas as pd, xgboost, numpy as np, textblob, string\n",
        "from keras.preprocessing import text, sequence\n",
        "from keras import layers, models, optimizers\n",
        "import re\n",
        "from unidecode import unidecode"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uc2vJ7_Li8ki",
        "colab_type": "code",
        "outputId": "1962199e-fd0b-445e-f2d8-dd08ad1b0824",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "import nltk \n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from urllib.parse import urlparse\n",
        "from nltk import word_tokenize"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YoVnzfOKlxPL",
        "colab_type": "code",
        "outputId": "a8474862-090b-46be-b8ac-0a811d0392c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "import nltk \n",
        "nltk.download('rslp')\n",
        "from nltk.stem import RSLPStemmer"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]   Unzipping stemmers/rslp.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q07HvZRCZVJ8",
        "colab_type": "code",
        "outputId": "a03f39db-1f29-4cae-947b-14efbb6a3254",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKFQTXqP8QEf",
        "colab_type": "text"
      },
      "source": [
        "### Pre Processamento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6k7_f69b8jO0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Constantes\n",
        "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
        "PONTUACTION = re.compile('[^\\w\\s]')\n",
        "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
        "STOPWORDS = set(stopwords.words('portuguese'))\n",
        "STOPWORDS2 = ['r', 'h', 'u', 'ub']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_YheRAACsrY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stemmer = RSLPStemmer()\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ISn7LomE70_B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Funcao pra remorar URL\n",
        "def is_url(url):\n",
        "  try:\n",
        "    result = urlparse(url)\n",
        "    return all([result.scheme, result.netloc])\n",
        "  except ValueError:\n",
        "    return False\n",
        "\n",
        "# Funcao para extrair o radical da palavra\n",
        "def stemming(text):\n",
        "  word_tokens = word_tokenize(text) \n",
        "  filtering = [stemmer.stem(w) for w in word_tokens]\n",
        "\n",
        "  text_final = ' '.join(filtering)\n",
        "  return text_final\n",
        "\n",
        "# tratamento lexical das palavras\n",
        "def lemmatizing(text):\n",
        "  list_filter = []\n",
        "  for word in text:\n",
        "    word_tokens = word_tokenize(word) \n",
        "    filtering = [lemmatizer.lemmatize(w) for w in word_tokens]\n",
        "    \n",
        "  text_final = ' '.join(filtering)\n",
        "  return text_final\n",
        "\n",
        "# remove palavras recorrente sem valor significativo\n",
        "def remove_palavras_recorrentes(text):\n",
        "  text = ' '.join(word2 for word2 in text.split() if word2 not in STOPWORDS2)\n",
        "  return text\n",
        "\n",
        "# pre processamento geral do texto\n",
        "def limpa_texto(text):\n",
        "    text = text.lower()\n",
        "    text = ' '.join(unidecode(word3) for word3 in text.split())\n",
        "    text = ' '.join(word for word in text.split() if word not in STOPWORDS)\n",
        "    text = REPLACE_BY_SPACE_RE.sub('', text)\n",
        "    text = BAD_SYMBOLS_RE.sub('', text)\n",
        "    text = PONTUACTION.sub('', text)\n",
        "    text = ''.join([i for i in text if not i.isdigit()])\n",
        "    text = text.strip()\n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOQVvazq73mg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# carrega o dataset\n",
        "df = pd.read_csv(\"train.csv\")\n",
        "\n",
        "# concatenei titulo e texto\n",
        "df = df.reset_index(drop=True)\n",
        "df['content'] = df['text'] + '\\n' + df['text']\n",
        "\n",
        "# remove URLs\n",
        "df['content'] = [' '.join(y for y in x.split() if not is_url(y)) for x in df['content']]\n",
        "\n",
        "# limpa texto\n",
        "df['content'] = df['content'].apply(limpa_texto)\n",
        "df['content'] = df['content'].apply(stemming)\n",
        "df['content'] = df['content'].apply(remove_palavras_recorrentes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vikhTq_HvVkb",
        "colab_type": "code",
        "outputId": "27a8bf80-ad7d-47e8-f63e-e70d255c2ae4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "source": [
        "trainDF = pd.DataFrame()\n",
        "trainDF['text'] = df.content\n",
        "trainDF['label'] = df.category\n",
        "Y_classes = pd.get_dummies(df['category']).columns\n",
        "\n",
        "trainDF.head()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>urban anarqu bairr antig lisbo escond real col...</td>\n",
              "      <td>turismo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>empr soc mostr possi unir negoci impact posi s...</td>\n",
              "      <td>empreendedorsocial</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>menos quatr estaco esqu centr chil vir obrig p...</td>\n",
              "      <td>turismo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>gravid provoc mudanc fisic durado cerebr mulh ...</td>\n",
              "      <td>ciencia</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>algum vez voc ja ouv fras o facebook ar so cel...</td>\n",
              "      <td>tec</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text               label\n",
              "0  urban anarqu bairr antig lisbo escond real col...             turismo\n",
              "1  empr soc mostr possi unir negoci impact posi s...  empreendedorsocial\n",
              "2  menos quatr estaco esqu centr chil vir obrig p...             turismo\n",
              "3  gravid provoc mudanc fisic durado cerebr mulh ...             ciencia\n",
              "4  algum vez voc ja ouv fras o facebook ar so cel...                 tec"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "na_OsK7a8a-7",
        "colab_type": "text"
      },
      "source": [
        "### Divisao do DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZKcvuzwOKoV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(trainDF['text'], trainDF['label'])\n",
        "\n",
        "# enconde das labels\n",
        "encoder = preprocessing.LabelEncoder()\n",
        "train_y = encoder.fit_transform(train_y)\n",
        "valid_y = encoder.fit_transform(valid_y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GcCv7G2-wj-h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# criaçãão do vetor de valores\n",
        "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
        "count_vect.fit(trainDF['text'])\n",
        "\n",
        "# transformacao em vetor os datasets de treino e teste\n",
        "xtrain_count =  count_vect.transform(train_x)\n",
        "xvalid_count =  count_vect.transform(valid_x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "So-sG2zYxcYX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#diversas metricas\n",
        "trainDF['char_count'] = trainDF['text'].apply(len)\n",
        "trainDF['word_count'] = trainDF['text'].apply(lambda x: len(x.split()))\n",
        "trainDF['word_density'] = trainDF['char_count'] / (trainDF['word_count']+1)\n",
        "trainDF['punctuation_count'] = trainDF['text'].apply(lambda x: len(\"\".join(_ for _ in x if _ in string.punctuation))) \n",
        "trainDF['title_word_count'] = trainDF['text'].apply(lambda x: len([wrd for wrd in x.split() if wrd.istitle()]))\n",
        "trainDF['upper_case_word_count'] = trainDF['text'].apply(lambda x: len([wrd for wrd in x.split() if wrd.isupper()]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4nrsCOzOUce",
        "colab_type": "code",
        "outputId": "d4ebf2b0-150a-4fe6-a0ed-1fdee96460d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "source": [
        "trainDF.head()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "      <th>char_count</th>\n",
              "      <th>word_count</th>\n",
              "      <th>word_density</th>\n",
              "      <th>punctuation_count</th>\n",
              "      <th>title_word_count</th>\n",
              "      <th>upper_case_word_count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>urban anarqu bairr antig lisbo escond real col...</td>\n",
              "      <td>turismo</td>\n",
              "      <td>4979</td>\n",
              "      <td>816</td>\n",
              "      <td>6.094247</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>empr soc mostr possi unir negoci impact posi s...</td>\n",
              "      <td>empreendedorsocial</td>\n",
              "      <td>1531</td>\n",
              "      <td>250</td>\n",
              "      <td>6.099602</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>menos quatr estaco esqu centr chil vir obrig p...</td>\n",
              "      <td>turismo</td>\n",
              "      <td>2897</td>\n",
              "      <td>520</td>\n",
              "      <td>5.560461</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>gravid provoc mudanc fisic durado cerebr mulh ...</td>\n",
              "      <td>ciencia</td>\n",
              "      <td>3173</td>\n",
              "      <td>508</td>\n",
              "      <td>6.233792</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>algum vez voc ja ouv fras o facebook ar so cel...</td>\n",
              "      <td>tec</td>\n",
              "      <td>1263</td>\n",
              "      <td>234</td>\n",
              "      <td>5.374468</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  ... upper_case_word_count\n",
              "0  urban anarqu bairr antig lisbo escond real col...  ...                     0\n",
              "1  empr soc mostr possi unir negoci impact posi s...  ...                     0\n",
              "2  menos quatr estaco esqu centr chil vir obrig p...  ...                     0\n",
              "3  gravid provoc mudanc fisic durado cerebr mulh ...  ...                     0\n",
              "4  algum vez voc ja ouv fras o facebook ar so cel...  ...                     0\n",
              "\n",
              "[5 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RrZApZWXz03a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lda_model = decomposition.LatentDirichletAllocation(n_components=20, learning_method='online', max_iter=20)\n",
        "X_topics = lda_model.fit_transform(xtrain_count)\n",
        "topic_word = lda_model.components_ \n",
        "vocab = count_vect.get_feature_names()\n",
        "\n",
        "# analise das principais palavras do texto\n",
        "n_top_words = 10\n",
        "topic_summaries = []\n",
        "for i, topic_dist in enumerate(topic_word):\n",
        "    topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
        "    topic_summaries.append(' '.join(topic_words))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Osz-x8WyYLO",
        "colab_type": "code",
        "outputId": "109b176f-e6c0-4a2c-d09a-c17fd2ad99bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        }
      },
      "source": [
        "topic_summaries"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['mulh sex violenc feminin gravid dor marid hormoni ginecolog desej',\n",
              " 'jog atlet futebol pinguim the batman kim titul burm peptide',\n",
              " 'medic paci trat saud doenc canc hospit drog test cas',\n",
              " 'abort ga oxigeni sall recolh recall carcac conversa branque tcu',\n",
              " 'fotograf fot imag viag tir selfi album juni edico registr',\n",
              " 'animal pesquis human celul gen dna gene vinh produz outr',\n",
              " 'noit brasil pesso aere inclu caf val hotel sao soc',\n",
              " 'ano especi agu pesquis are pod outr terr sol viru',\n",
              " 'curs estud ano prov vag en ensin ser not univers',\n",
              " 'sao cidad nao cas ond fic visit ano local ha',\n",
              " 'aliment com pes consum gord obes diet acuc alimentaca sauda',\n",
              " 'carr sao motor empr veicul ser cust model paul mil',\n",
              " 'dia fas vestibul prim prov candidat segund list univers questo',\n",
              " 'empr us usuari diss nao nov ser ano serv red',\n",
              " 'cerebr beb son estud pesquis pel doenc pod cient corp',\n",
              " 'russ bact antibio dron contribuica resist uerj schmidt min resistenc',\n",
              " 'carn porc g butantan produt protoindoeurop rotul food yehud burg',\n",
              " 'escol nao alun educaca ano ensin govern profes estud paul',\n",
              " 'lixa unboxing autist eataly confeder auxili melani arcoiril saramp tdi',\n",
              " 'nao pod diz ser pesso faz outr ano sao trabalh']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ISSwCqrL3y3",
        "colab_type": "text"
      },
      "source": [
        "###Definicoes do Modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJO0f-zMwtX9",
        "colab_type": "code",
        "outputId": "e901a978-f594-4029-afb5-e5a2a387d282",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "value_max_feature = 7000\n",
        "# word level tf-idf\n",
        "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=value_max_feature)\n",
        "tfidf_vect.fit(trainDF['text'])\n",
        "xtrain_tfidf =  tfidf_vect.transform(train_x)\n",
        "xvalid_tfidf =  tfidf_vect.transform(valid_x)\n",
        "\n",
        "# ngram level tf-idf \n",
        "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=value_max_feature)\n",
        "tfidf_vect_ngram.fit(trainDF['text'])\n",
        "xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(train_x)\n",
        "xvalid_tfidf_ngram =  tfidf_vect_ngram.transform(valid_x)\n",
        "\n",
        "# characters level tf-idf\n",
        "tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=value_max_feature)\n",
        "tfidf_vect_ngram_chars.fit(trainDF['text'])\n",
        "xtrain_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(train_x) \n",
        "xvalid_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(valid_x) "
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:524: UserWarning: The parameter 'token_pattern' will not be used since 'analyzer' != 'word'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YB4qUIKV8rS7",
        "colab_type": "text"
      },
      "source": [
        "### Criacao dos Embedding do Texto"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_xo00nV2q4Y",
        "colab_type": "code",
        "outputId": "1a4de7ad-944f-49cd-d4ec-1219228268c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        }
      },
      "source": [
        "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.pt.300.vec.gz"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-03-18 00:42:49--  https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.pt.300.vec.gz\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.20.6.166, 104.20.22.166, 2606:4700:10::6814:16a6, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.20.6.166|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1271093660 (1.2G) [binary/octet-stream]\n",
            "Saving to: ‘cc.pt.300.vec.gz’\n",
            "\n",
            "cc.pt.300.vec.gz    100%[===================>]   1.18G  12.3MB/s    in 1m 41s  \n",
            "\n",
            "2020-03-18 00:44:31 (12.0 MB/s) - ‘cc.pt.300.vec.gz’ saved [1271093660/1271093660]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rh3Qg2VJ3Mt9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!gzip -d cc.pt.300.vec.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-VKMuatE3vlP",
        "colab_type": "code",
        "outputId": "34438e34-5229-468b-bc9a-1f4805a2ae37",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " cc.pt.300.vec\t\t\t\t\t\t    test.csv\n",
            "'download.php?file=embeddings%2Fword2vec%2Fcbow_s300.zip'   train.csv\n",
            " sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cNE3Acd0xBc7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# carregamento do embedding pre treinado\n",
        "embeddings_index = {}\n",
        "for i, line in enumerate(open('cc.pt.300.vec')):\n",
        "    values = line.split()\n",
        "    embeddings_index[values[0]] = np.asarray(values[1:], dtype='float32')\n",
        "\n",
        "# criacao dos tokens\n",
        "token = text.Tokenizer()\n",
        "token.fit_on_texts(trainDF['text'])\n",
        "word_index = token.word_index\n",
        "\n",
        "# converter texto em sequencia de vetores\n",
        "train_seq_x = sequence.pad_sequences(token.texts_to_sequences(train_x), maxlen=70)\n",
        "valid_seq_x = sequence.pad_sequences(token.texts_to_sequences(valid_x), maxlen=70)\n",
        "\n",
        "# mapeamento do texto com o embedding\n",
        "embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ka7wvjPm85Xc",
        "colab_type": "text"
      },
      "source": [
        "### Criacao dos Modelos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pmb9i8ZX5kQX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model(classifier, feature_vector_train, label, feature_vector_valid, is_neural_net=False):\n",
        "    classifier.fit(feature_vector_train, label)\n",
        "    predictions = classifier.predict(feature_vector_valid)\n",
        "    \n",
        "    if is_neural_net:\n",
        "        predictions = predictions.argmax(axis=-1)\n",
        "        \n",
        "    return  classifier, metrics.f1_score(valid_y, predictions, average='weighted')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CR207wwL566s",
        "colab_type": "code",
        "outputId": "363ab063-0621-42ae-a94b-b111a42e65f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "# Naive Bayes on Count Vectors\n",
        "base_model, accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_count, train_y, xvalid_count)\n",
        "print(\"NB, Count Vectors: \", accuracy)\n",
        "\n",
        "# Naive Bayes on Word Level TF IDF Vectors\n",
        "base_model, accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
        "print(\"NB, WordLevel TF-IDF: \", accuracy)\n",
        "\n",
        "# Naive Bayes on Ngram Level TF IDF Vectors\n",
        "base_model, accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
        "print(\"NB, N-Gram Vectors: \", accuracy)\n",
        "\n",
        "# Naive Bayes on Character Level TF IDF Vectors\n",
        "base_model, accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars)\n",
        "print(\"NB, CharLevel Vectors: \", accuracy)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NB, Count Vectors:  0.9034749384151586\n",
            "NB, WordLevel TF-IDF:  0.821393493574874\n",
            "NB, N-Gram Vectors:  0.7948897679717226\n",
            "NB, CharLevel Vectors:  0.5083857663372462\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQx10AqC7aJt",
        "colab_type": "code",
        "outputId": "20d5e1e8-4a18-4ec9-f7f2-7e083b062bed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        }
      },
      "source": [
        "# Linear Classifier on Count Vectors\n",
        "base_model, accuracy = train_model(linear_model.LogisticRegression(random_state=1, max_iter=200, class_weight='balanced'), xtrain_count, train_y, xvalid_count)\n",
        "print(\"LR, Count Vectors: \", accuracy)\n",
        "\n",
        "# Linear Classifier on Word Level TF IDF Vectors\n",
        "base_model, accuracy = train_model(linear_model.LogisticRegression(random_state=1, class_weight='balanced', C=2.0), xtrain_tfidf, train_y, xvalid_tfidf)\n",
        "print(\"LR, WordLevel TF-IDF: \", accuracy)\n",
        "\n",
        "# Linear Classifier on Ngram Level TF IDF Vectors\n",
        "base_model, accuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
        "print(\"LR, N-Gram Vectors: \", accuracy)\n",
        "\n",
        "# Linear Classifier on Character Level TF IDF Vectors\n",
        "base_model, accuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars)\n",
        "print(\"LR, CharLevel Vectors: \", accuracy)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "LR, Count Vectors:  0.8878176209060492\n",
            "LR, WordLevel TF-IDF:  0.9159347017018749\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "LR, N-Gram Vectors:  0.8173047187194916\n",
            "LR, CharLevel Vectors:  0.8708550547389966\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t0zaGUPo_dyj",
        "colab_type": "code",
        "outputId": "c7f1a193-a11f-4a99-85f1-accdd1350b14",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "# SVM Classifier on Count Vectors\n",
        "base_model, accuracy = train_model(svm.SVC(), xtrain_count, train_y, xvalid_count)\n",
        "print(\"LR, Count Vectors: \", accuracy)\n",
        "\n",
        "# SVM Classifier on Word Level TF IDF Vectors\n",
        "base_model, accuracy = train_model(svm.SVC(C=2, class_weight='balanced'), xtrain_tfidf, train_y, xvalid_tfidf)\n",
        "print(\"LR, WordLevel TF-IDF: \", accuracy)\n",
        "\n",
        "# SVM on Ngram Level TF IDF Vectors\n",
        "base_model, accuracy = train_model(svm.SVC(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
        "print(\"SVM, N-Gram Vectors: \", accuracy)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LR, Count Vectors:  0.8704933600442418\n",
            "LR, WordLevel TF-IDF:  0.9114085550020787\n",
            "SVM, N-Gram Vectors:  0.8063254554658117\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKQSmaPEYuER",
        "colab_type": "code",
        "outputId": "ee2eca9b-ff14-4561-e69a-2f2107f33a5b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# RF on Count Vectors\n",
        "base_model, accuracy = train_model(ensemble.RandomForestClassifier(), xtrain_count, train_y, xvalid_count)\n",
        "print(\"RF, Count Vectors: \", accuracy)\n",
        "\n",
        "# RF on Word Level TF IDF Vectors\n",
        "base_model, accuracy = train_model(ensemble.RandomForestClassifier(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
        "print(\"RF, WordLevel TF-IDF: \", accuracy)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RF, Count Vectors:  0.8348484285986371\n",
            "RF, WordLevel TF-IDF:  0.8515364828693002\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1xEPmrKZBpq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "c324113d-38ae-47c3-944a-0d718e1af41d"
      },
      "source": [
        "# Extereme Gradient Boosting on Count Vectors\n",
        "base_model, accuracy = train_model(xgboost.XGBClassifier(), xtrain_count.tocsc(), train_y, xvalid_count.tocsc())\n",
        "print( \"Xgb, Count Vectors: \", accuracy)\n",
        "\n",
        "# Extereme Gradient Boosting on Word Level TF IDF Vectors\n",
        "base_model, accuracy = train_model(xgboost.XGBClassifier(), xtrain_tfidf.tocsc(), train_y, xvalid_tfidf.tocsc())\n",
        "print( \"Xgb, WordLevel TF-IDF: \", accuracy)\n",
        "\n",
        "# Extereme Gradient Boosting on Character Level TF IDF Vectors\n",
        "base_model, accuracy = train_model(xgboost.XGBClassifier(), xtrain_tfidf_ngram_chars.tocsc(), train_y, xvalid_tfidf_ngram_chars.tocsc())\n",
        "print( \"Xgb, CharLevel Vectors: \", accuracy)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Xgb, Count Vectors:  0.8772081867294187\n",
            "Xgb, WordLevel TF-IDF:  0.8746564297832835\n",
            "Xgb, CharLevel Vectors:  0.8472514601644491\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w5zl4ZKZBNPH",
        "colab_type": "code",
        "outputId": "7390f789-ed23-4d45-f530-db12bcc3e571",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        " # SGD Classifier on Count Vectors\n",
        "base_model, accuracy = train_model(linear_model.SGDClassifier(max_iter=1000, tol=1e-3), xtrain_count, train_y, xvalid_count)\n",
        "print(\"SGD, Count Vectors: \", accuracy)\n",
        "\n",
        "# SGD Classifier on Word Level TF IDF Vectors\n",
        "base_model, accuracy = train_model(linear_model.SGDClassifier(max_iter=2000, tol=1e-2), xtrain_tfidf, train_y, xvalid_tfidf)\n",
        "print(\"SGD, WordLevel TF-IDF: \", accuracy)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SGD, Count Vectors:  0.8855914969599166\n",
            "SGD, WordLevel TF-IDF:  0.9119467663816223\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0sr4xVqlqZxS",
        "colab_type": "text"
      },
      "source": [
        "##Ensemble Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fdvIF2EqYhw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "clf1 = LogisticRegression(random_state=1, class_weight='balanced', C=2.0)\n",
        "clf2 = svm.SVC(C=2, class_weight='balanced')\n",
        "clf3 = linear_model.SGDClassifier(max_iter=2000, tol=1e-2)\n",
        "\n",
        "eclf = VotingClassifier(\n",
        "    estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)],\n",
        "    voting='hard')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQpaMRkjTX4v",
        "colab_type": "code",
        "outputId": "2cd1f712-f7a3-4a65-e370-4b1c5d96022b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "source": [
        "# Linear Classifier on Count Vectors\n",
        "base_model, accuracy = train_model(eclf, xtrain_count, train_y, xvalid_count)\n",
        "print(\"LR, Count Vectors: \", accuracy)\n",
        "\n",
        "base_model, accuracy = train_model(eclf, xtrain_tfidf, train_y, xvalid_tfidf)\n",
        "print(\"LR, WordLevel TF-IDF: \", accuracy)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "LR, Count Vectors:  0.8974643819169975\n",
            "LR, WordLevel TF-IDF:  0.9156799334125799\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOttoQjtqYub",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "65ed839f-2482-4b41-90b7-89df1896850c"
      },
      "source": [
        "# Extra Trees Classification (Bagging)\n",
        "import pandas\n",
        "from sklearn import model_selection\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "\n",
        "seed = 7\n",
        "num_trees = 100\n",
        "max_features = 7\n",
        "kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
        "model = ExtraTreesClassifier(n_estimators=num_trees)\n",
        "results = model_selection.cross_val_score(model, xtrain_tfidf, train_y, cv=kfold)\n",
        "print(results.mean())"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
            "  FutureWarning\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.8471283783783783\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUCf7oQUJUdL",
        "colab_type": "code",
        "outputId": "0c169ee1-f299-4165-e65d-0f8f1b986f33",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "# AdaBoost Classification (Boosting)\n",
        "import pandas\n",
        "from sklearn import model_selection\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "seed = 1\n",
        "num_trees = 30\n",
        "kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
        "model = AdaBoostClassifier(n_estimators=num_trees, random_state=seed)\n",
        "results = model_selection.cross_val_score(model, xtrain_count, train_y, cv=kfold)\n",
        "print(results.mean())"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
            "  FutureWarning\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.7118243243243243\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dSuhPzDmJf2j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "2100735d-0dcc-4f92-91cf-0cddf001efff"
      },
      "source": [
        "# Stochastic Gradient Boosting Classification (Boosting)\n",
        "import pandas\n",
        "from sklearn import model_selection\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "seed = 42\n",
        "num_trees = 100\n",
        "kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
        "model = GradientBoostingClassifier(n_estimators=num_trees, random_state=seed)\n",
        "results = model_selection.cross_val_score(model, xtrain_tfidf, train_y, cv=kfold)\n",
        "print(results.mean())"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
            "  FutureWarning\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.8618243243243244\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6nCM9KgAct9",
        "colab_type": "text"
      },
      "source": [
        "### Validacao e criacao arquivo submissao"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4b3Jse7FAhgj",
        "colab_type": "code",
        "outputId": "a7f5f94e-d4ef-409e-9781-46b886dcd99f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "# Leitura do Dataset de validação dos resultados\n",
        "test_df = pd.read_csv('test.csv')\n",
        "print(test_df.shape)\n",
        "test_df.head()"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4251, 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>article_id</th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4763</td>\n",
              "      <td>Enem 2016 aumenta 46% o uso do nome social por...</td>\n",
              "      <td>O número de travestis e transexuais que usará ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>52</td>\n",
              "      <td>'Viagem ao Japão é aula de cultura e tradição'...</td>\n",
              "      <td>O ator Jayme Matarazzo, 31, aproveita os inter...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>7682</td>\n",
              "      <td>Fotógrafo registra a beleza natural de países ...</td>\n",
              "      <td>O fotógrafo Vitor Schietti, 29, passou quase u...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>10292</td>\n",
              "      <td>Azar genético explica preferência do Aedes aeg...</td>\n",
              "      <td>Enquanto alguns sofrem, outros escapam incólum...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7435</td>\n",
              "      <td>Parto humanizado e capital humano ganham apoio...</td>\n",
              "      <td>A Womanity Foundation anunciou no início do mê...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   article_id  ...                                               text\n",
              "0        4763  ...  O número de travestis e transexuais que usará ...\n",
              "1          52  ...  O ator Jayme Matarazzo, 31, aproveita os inter...\n",
              "2        7682  ...  O fotógrafo Vitor Schietti, 29, passou quase u...\n",
              "3       10292  ...  Enquanto alguns sofrem, outros escapam incólum...\n",
              "4        7435  ...  A Womanity Foundation anunciou no início do mê...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BhprxYNRjDxV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_df = test_df.reset_index(drop=True)\n",
        "test_df['content'] =  test_df['text'] + '\\n' + test_df['text']\n",
        "\n",
        "test_df['content'] = [' '.join(y for y in x.split() if not is_url(y)) for x in test_df['content']]\n",
        "\n",
        "test_df['content'] = test_df['content'].apply(limpa_texto)\n",
        "\n",
        "test_df['content'] = test_df['content'].apply(stemming)\n",
        "\n",
        "test_df['content'] = test_df['content'].apply(remove_palavras_recorrentes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJ-1hoUBBEYb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict():\n",
        "    new_text = tfidf_vect.transform(test_df.content)\n",
        "    pred     = base_model.predict(new_text)\n",
        "    return pred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y5D8hmnsCK_X",
        "colab_type": "code",
        "outputId": "79f420a9-df97-43d6-9c86-ee91745c751f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "pred         = predict()\n",
        "pred_classes = [Y_classes[c] for c in pred]\n",
        "pred_classes[:5]"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['educacao', 'turismo', 'turismo', 'ciencia', 'empreendedorsocial']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L4AQ1cIeCR_R",
        "colab_type": "code",
        "outputId": "22362170-4ede-466e-99c5-118e4447f050",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "source": [
        "# Atualizando a categoria dos artigos no dataset de validação\n",
        "test_df['category'] = pred_classes\n",
        "test_df.head()"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>article_id</th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>content</th>\n",
              "      <th>category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4763</td>\n",
              "      <td>Enem 2016 aumenta 46% o uso do nome social por...</td>\n",
              "      <td>O número de travestis e transexuais que usará ...</td>\n",
              "      <td>numer travestil transex us nom soc en exam nac...</td>\n",
              "      <td>educacao</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>52</td>\n",
              "      <td>'Viagem ao Japão é aula de cultura e tradição'...</td>\n",
              "      <td>O ator Jayme Matarazzo, 31, aproveita os inter...</td>\n",
              "      <td>at jaym matarazz aproveit interval gravaco via...</td>\n",
              "      <td>turismo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>7682</td>\n",
              "      <td>Fotógrafo registra a beleza natural de países ...</td>\n",
              "      <td>O fotógrafo Vitor Schietti, 29, passou quase u...</td>\n",
              "      <td>fotograf vit schiett pass quas me viaj quatr p...</td>\n",
              "      <td>turismo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>10292</td>\n",
              "      <td>Azar genético explica preferência do Aedes aeg...</td>\n",
              "      <td>Enquanto alguns sofrem, outros escapam incólum...</td>\n",
              "      <td>enquant algum sofr outr escap incolum comum qu...</td>\n",
              "      <td>ciencia</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7435</td>\n",
              "      <td>Parto humanizado e capital humano ganham apoio...</td>\n",
              "      <td>A Womanity Foundation anunciou no início do mê...</td>\n",
              "      <td>womanity foundation anunci inici me nov fellow...</td>\n",
              "      <td>empreendedorsocial</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   article_id  ...            category\n",
              "0        4763  ...            educacao\n",
              "1          52  ...             turismo\n",
              "2        7682  ...             turismo\n",
              "3       10292  ...             ciencia\n",
              "4        7435  ...  empreendedorsocial\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjk_zffICr0g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#criacao do arquivo de submissao para a competicao\n",
        "test_df[[\"article_id\", \"category\"]].to_csv(\"submission.csv\", index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}